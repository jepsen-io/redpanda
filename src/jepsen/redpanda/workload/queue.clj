(ns jepsen.redpanda.workload.queue
  "A workload which treats Kafka more as a queue. Each client maintains a
  producer and consumer. To subscribe to a new set of topics, we issue an
  operation like:

    {:f :subscribe, :value [topic1, topic2, ...]}

  Just like the Kafka API, this replaces the topic listing entirely.

  Reads and writes (and mixes thereof) are encoded as a vector of
  micro-operations:

    {:f :read,  :value [op1, op2, ...]}
    {:f :write, :value [op1, op2, ...]}
    {:f :txn,   :value [op1, op2, ...]}

  Where :read and :write denote transactions comprising only reads or writes,
  respectively, and :txn indicates a general-purpose transaction. Operations
  are of two forms:

    [:send key value]

  ... instructs a client to append `value` to the integer `key`--which maps
  uniquely to a single topic and partition. These operations are returned as:

    [:send key [offset value]]

  where offset is the returned offset of the write, if available, or `nil` if
  it is unknown (e.g. if the write times out).

  Reads are invoked as:

    [:poll]

  ... which directs the client to perform a single `poll` operation on its
  consumer. The results of that poll are expanded to:

    [:poll {key1 [[offset1 value1] [offset2 value2] ...],
            key2 [...]}]

  Where key1, key2, etc are integer keys obtained from the topic-partitions
  returned by the call to poll, and the value for that key is a vector of
  [offset value] pairs, corresponding to the offset of that message in that
  particular topic-partition, and the value of the message---presumably,
  whatever was written by `[:send key value]` earlier.

  Before a transaction completes, we commit its offsets.

  From this history we can perform a number of analyses:

  1. For any observed value of a key, we check to make sure that its writer was
  either :ok or :info; if the writer :failed, we know this constitutes an
  aborted read.

  2. We verify that all sends and polls agree on the value for a given key and
  offset. We do not require contiguity in offsets, because transactions add
  invisible messages which take up an offset slot but are not visible to the
  API. If we find divergence, we know that Kakfa disagreed about the value at
  some offset.

  Having verified that each [key offset] pair uniquely identifies a single
  value, we eliminate the offsets altogether and perform the remainder of the
  analysis purely in terms of keys and values. We construct a graph where
  vertices are values, and an edge v1 -> v2 means that v1 immediately precedes
  v2 in the offset order (ignoring gaps in the offsets, which we assume are due
  to transaction metadata messages).

  3. For each key, we take the highest observed offset, and then check that
  every :ok :send operation with an equal or lower offset was *also* read by at
  least one consumer. If we find one, we know a write was lost!

  4. We build a dependency graph between pairs of transactions T1 and T2, where
  T1 != T2, like so:

    ww. T1 sent value v1 to key k, and T2 sent v2 to k, and o1 < o2
        in the version order for k.

    wr. T1 sent v1 to k, and T2's highest read of k was v1.

    rw. T1's highest read of key k was offset o1, and T2 sent offset o2 to k,
        and o1 < o2 in the version order for k.

  Our use of \"highest offset\" is intended to capture the fact that each poll
  operation observes a *range* of offsets, but in general those offsets could
  have been generated by *many* transactions. If we drew wr edges for every
  offset polled, we'd generate superfluous edges--all writers are already
  related via ww dependencies, so the final wr edge, plus those ww edges,
  captures those earlier read values.

  We draw rw edges only for the final versions of each key observed by a
  transaction. If we drew rw edges for an earlier version, we would incorrectly
  be asserting that later transactions were *not* observed!

  We perform cycle detection and categorization of anomalies from this graph
  using Elle.

  5. Internal Read Contiguity: Within a transaction, each pair of reads on the
  same key should be directly related in the version order. If we observe a gap
  (e.g. v1 < ... < v2) that indicates this transaction skipped over some
  values. If we observe an inversion (e.g. v2 < v1, or v2 < ... < v1) then we
  know that the transaction observed an order which disagreed with the \"true\"
  order of the log.

  6. Internal Write Contiguity: Gaps between sequential pairs of writes to the
  same key are detected via Elle as write cycles. Inversions are not, so we
  check for them explicitly: a transaction sends v1, then v2, but v2 < v1 or v2
  < ... v1 in the version order.

  6. Intermediate reads? I assume these happen constantly, but are they
  supposed to? It's not totally clear what this MEANS, but I think it might
  look like a transaction T1 which writes [v1 v2 v3] to k, and another T2 which
  polls k and observes any of v1, v2, or v3, but not *all* of them. This
  miiight be captured as a wr-rw cycle in some cases, but perhaps not all,
  since we're only generating rw edges for final reads."
  (:require [clojure.tools.logging :refer [info warn]]
            [dom-top.core :refer [assert+]]
            [jepsen [client :as client]
                    [generator :as gen]
                    [util :as util :refer [pprint-str]]]
            [jepsen.tests.cycle.append :as append]
            [jepsen.redpanda [client :as rc]])
  (:import (java.util.concurrent ExecutionException)
           (org.apache.kafka.clients.consumer ConsumerRecord)
           (org.apache.kafka.common.errors InvalidTopicException
                                           NotLeaderOrFollowerException
                                           TimeoutException
                                           UnknownTopicOrPartitionException
                                           )))

; Just wrote the documentation here; the code is a WIP, bears no relation to
; the actual plan, and will likely be mostly rewritten.

(def partition-count
  "How many partitions per topic?"
  2)

(defn k->topic
  "Turns a logical key into a topic."
  [k]
  (str "t" (quot k partition-count)))

(defn k->partition
  "Turns a logical key into a partition within a topic."
  [k]
  (mod k partition-count))

(defn k->topic-partition
  "Turns a logical key into a TopicPartition."
  [k]
  (rc/topic-partition (k->topic k) (k->partition k)))

(def replication-factor
  "What replication factor should we use for each topic?"
  3)

(defn mop!
  "Applies a micro-operation from a transaction: either a :r read or a :append
  operation."
  [{:keys [extant-topics consumed admin producer consumer] :as client}
   [f k v :as mop]]
  (let [topic           (k->topic k)
        topic-partition (k->topic-partition k)]
    (case f
      :r
      (try
        ; Start by assigning our consumer to this particular topic, seeking
        ; the consumer to the beginning, then reading the entire topic.
        (doto consumer
          (.assign [topic-partition])
          (.seekToBeginning [topic-partition]))

        ; How far do we have to read?
        (let [end-offset (-> consumer
                             (.endOffsets [topic-partition])
                             (get topic-partition))
              ; Read at least that far
              records (rc/poll-up-to consumer end-offset)
              ; Map records back into a list of integer elements
              elements (mapv (fn record->element [^ConsumerRecord r]
                               (.value r))
                             records)]
          [f k elements])
        (catch InvalidTopicException _
          ; This can happen when a topic is created on one side of a partition
          ; but another node doesn't know about it yet.
          [f k nil]))

      :append
      (do ; Create topic if it doesn't exist.
          (when-not (contains? @extant-topics topic)
            (rc/create-topic! admin topic partition-count replication-factor)
            (swap! extant-topics conj topic)))

      (let [record (rc/producer-record topic (k->partition k) nil v)
            res    @(.send producer record)]
        mop))))

(defrecord Client [; Our three Kafka clients
                   admin producer consumer
                   ; A collection of topics we're subscribed to.
                   subscriptions
                   ; An atom with a set of topics we've created. We have to
                   ; create topics before they can be used.
                   extant-topics
                   ; An atom of a map of keys to vectors of elements: the last
                   ; observed values for each key. When performing a read of a
                   ; key, we append any consumed elements onto the
                   ; corresponding vector here to determine the full read,
                   ; allowing us to transform a simple produce/consume workload
                   ; into a list-append history.
                   consumed]
  client/Client
  (open! [this test node]
    (let [tx-id (rc/new-transactional-id)]
      (assoc this
             :admin     (rc/admin test node)
             :producer  (rc/producer (assoc test :transactional-id tx-id))
             :consumer  (rc/consumer test node))))

  (setup! [this test])

  (invoke! [this test op]
    (try
      (let [txn  (:value op)
            txn' (mapv (partial mop! this) txn)]
        (assoc op :type :ok, :value txn'))
      (catch ExecutionException e
        (condp instance? (util/ex-root-cause e)
          InvalidTopicException
          (assoc op :type :fail, :error :invalid-topic)

          NotLeaderOrFollowerException
          (assoc op :type :fail, :error :not-leader-or-follower)

          ; Love that we have to catch this in two different ways
          TimeoutException
          (assoc op :type :info, :error :timeout)

          UnknownTopicOrPartitionException
          (assoc op :type :fail, :error :unknown-topic-or-partition)

          (throw e)))
      (catch TimeoutException e
        (assoc op :type :info, :error :timeout))))

  (teardown! [this test])

  (close! [this test]
    (rc/close! admin)
    (rc/close! producer)
    (rc/close! consumer)))

(defn client
  "Constructs a fresh client for this workload."
  []
  (map->Client {:extant-topics (atom #{})}))

(defn workload
  "Constructs a workload (a map with a generator, client, checker, etc) given
  an options map. Options are:

    (none)

  ... plus those taken by jepsen.tests.cycle.append/test, e.g. :key-count,
  :min-txn-length, ..."
  [opts]
  (let [workload (append/test
                   (assoc opts
                          ; TODO: don't hardcode these
                          :max-txn-length 1
                          :consistency-models [:strict-serializable]))]
    (-> workload
        (assoc :client (client))
        ; Rewrite generator ops to use :f :read or :f :write if they're read or
        ; write-only. Elle doesn't care, but this helps us visualize read vs
        ; write perf better.
        (update :generator
                (fn wrap-gen [gen]
                  (gen/map (fn tag-rw [op]
                             (case (->> op :value (map first) set)
                               #{:r}      (assoc op :f :read)
                               #{:append} (assoc op :f :write)
                               op))
                           gen))))))
