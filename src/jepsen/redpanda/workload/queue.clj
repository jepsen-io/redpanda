(ns jepsen.redpanda.workload.queue
  "A workload which treats Redpanda more as a queue. Each client maintains a
  producer and consumer. To subscribe to a new set of topics, we issue an
  operation like:

    {:f :subscribe, :value [k1, k2, ...]}

  or

    {:f :assign, :value [k1, k2, ...]}

  ... where k1, k2, etc denote specific topics and partitions. For subscribe,
  we just use that key's topic, and allow Redpanda to control which partitions
  we get. Just like the Kafka client API, both subscribe and assign replace the
  current topics for the consumer.

  Reads and writes (and mixes thereof) are encoded as a vector of
  micro-operations:

    {:f :poll, :value [op1, op2, ...]}
    {:f :send, :value [op1, op2, ...]}
    {:f :txn,  :value [op1, op2, ...]}

  Where :poll and :send denote transactions comprising only reads or writes,
  respectively, and :txn indicates a general-purpose transaction. Operations
  are of two forms:

    [:send key value]

  ... instructs a client to append `value` to the integer `key`--which maps
  uniquely to a single topic and partition. These operations are returned as:

    [:send key [offset value]]

  where offset is the returned offset of the write, if available, or `nil` if
  it is unknown (e.g. if the write times out).

  Reads are invoked as:

    [:poll]

  ... which directs the client to perform a single `poll` operation on its
  consumer. The results of that poll are expanded to:

    [:poll {key1 [[offset1 value1] [offset2 value2] ...],
            key2 [...]}]

  Where key1, key2, etc are integer keys obtained from the topic-partitions
  returned by the call to poll, and the value for that key is a vector of
  [offset value] pairs, corresponding to the offset of that message in that
  particular topic-partition, and the value of the message---presumably,
  whatever was written by `[:send key value]` earlier.

  Before a transaction completes, we commit its offsets.

  From this history we can perform a number of analyses:

  1. For any observed value of a key, we check to make sure that its writer was
  either :ok or :info; if the writer :failed, we know this constitutes an
  aborted read.

  2. We verify that all sends and polls agree on the value for a given key and
  offset. We do not require contiguity in offsets, because transactions add
  invisible messages which take up an offset slot but are not visible to the
  API. If we find divergence, we know that Kakfa disagreed about the value at
  some offset.

  Having verified that each [key offset] pair uniquely identifies a single
  value, we eliminate the offsets altogether and perform the remainder of the
  analysis purely in terms of keys and values. We construct a graph where
  vertices are values, and an edge v1 -> v2 means that v1 immediately precedes
  v2 in the offset order (ignoring gaps in the offsets, which we assume are due
  to transaction metadata messages).

  3. For each key, we take the highest observed offset, and then check that
  every :ok :send operation with an equal or lower offset was *also* read by at
  least one consumer. If we find one, we know a write was lost!

  4. We build a dependency graph between pairs of transactions T1 and T2, where
  T1 != T2, like so:

    ww. T1 sent value v1 to key k, and T2 sent v2 to k, and o1 < o2
        in the version order for k.

    wr. T1 sent v1 to k, and T2's highest read of k was v1.

    rw. T1's highest read of key k was offset o1, and T2 sent offset o2 to k,
        and o1 < o2 in the version order for k.

  Our use of \"highest offset\" is intended to capture the fact that each poll
  operation observes a *range* of offsets, but in general those offsets could
  have been generated by *many* transactions. If we drew wr edges for every
  offset polled, we'd generate superfluous edges--all writers are already
  related via ww dependencies, so the final wr edge, plus those ww edges,
  captures those earlier read values.

  We draw rw edges only for the final versions of each key observed by a
  transaction. If we drew rw edges for an earlier version, we would incorrectly
  be asserting that later transactions were *not* observed!

  We perform cycle detection and categorization of anomalies from this graph
  using Elle.

  5. Internal Read Contiguity: Within a transaction, each pair of reads on the
  same key should be directly related in the version order. If we observe a gap
  (e.g. v1 < ... < v2) that indicates this transaction skipped over some
  values. If we observe an inversion (e.g. v2 < v1, or v2 < ... < v1) then we
  know that the transaction observed an order which disagreed with the \"true\"
  order of the log.

  6. Internal Write Contiguity: Gaps between sequential pairs of writes to the
  same key are detected via Elle as write cycles. Inversions are not, so we
  check for them explicitly: a transaction sends v1, then v2, but v2 < v1 or v2
  < ... v1 in the version order.

  6. Intermediate reads? I assume these happen constantly, but are they
  supposed to? It's not totally clear what this MEANS, but I think it might
  look like a transaction T1 which writes [v1 v2 v3] to k, and another T2 which
  polls k and observes any of v1, v2, or v3, but not *all* of them. This
  miiight be captured as a wr-rw cycle in some cases, but perhaps not all,
  since we're only generating rw edges for final reads."
  (:require [clojure [set :as set]]
            [clojure.tools.logging :refer [info warn]]
            [dom-top.core :refer [assert+]]
            [jepsen [checker :as checker]
                    [client :as client]
                    [generator :as gen]
                    [store :as store]
                    [util :as util :refer [map-vals
                                           nanos->secs
                                           parse-long
                                           pprint-str]]]
            [jepsen.checker.perf :as perf]
            [jepsen.tests.cycle.append :as append]
            [jepsen.redpanda [client :as rc]
                             [db :as db]]
            [knossos [history :as history]]
            [slingshot.slingshot :refer [try+ throw+]])
  (:import (java.util.concurrent ExecutionException)
           (org.apache.kafka.clients.admin Admin)
           (org.apache.kafka.clients.consumer ConsumerRecords
                                              ConsumerRecord
                                              KafkaConsumer)
           (org.apache.kafka.clients.producer KafkaProducer
                                              RecordMetadata)
           (org.apache.kafka.common TopicPartition)
           (org.apache.kafka.common.errors DisconnectException
                                           InvalidTopicException
                                           InvalidReplicationFactorException
                                           NetworkException
                                           NotControllerException
                                           NotLeaderOrFollowerException
                                           TimeoutException
                                           UnknownTopicOrPartitionException
                                           UnknownServerException
                                           )))

(def partition-count
  "How many partitions per topic?"
  2)

(def replication-factor
  "What replication factor should we use for each topic?"
  3)

(def poll-ms
  "How long should we poll for, in ms?"
  100)

(defn k->topic
  "Turns a logical key into a topic."
  [k]
  (str "t" (quot k partition-count)))

(defn k->partition
  "Turns a logical key into a partition within a topic."
  [k]
  (mod k partition-count))

(defn k->topic-partition
  "Turns a logical key into a TopicPartition."
  [k]
  (rc/topic-partition (k->topic k) (k->partition k)))

(defn topic-partition->k
  "Turns a TopicPartition into a key."
  ([^TopicPartition tp]
   (topic-partition->k (.topic tp) (.partition tp)))
  ([topic partition]
   (+ (* partition-count (parse-long (nth (re-find #"t(\d+)" topic) 1)))
      partition)))

(defn mop!
  "Applies a micro-operation from a transaction: either a :r read or a :append
  operation."
  [{:keys [extant-topics
           ^Admin admin
           ^KafkaProducer producer
           ^KafkaConsumer consumer] :as client}
   poll-ms
   mop]
  (case (first mop)
    :poll (try
            (rc/unwrap-errors
              (let [records (.poll consumer (rc/ms->duration poll-ms))]
                (->> (.partitions records)
                     (map (fn per-topic-partition [topic-partition]
                            ; Return a key, messages pair
                            [(topic-partition->k topic-partition)
                             ; Turn each message into an offset, record] pair
                             (mapv (fn xform-messages [^ConsumerRecord record]
                                     [(.offset record)
                                      (.value record)])
                                   (.records records topic-partition))]))
                     (into (sorted-map))
                     (vector :poll))))
              (catch InvalidTopicException _
                [:poll {}])
              (catch IllegalStateException e
                (if (re-find #"not subscribed to any" (.getMessage e))
                  [:poll {}]
                  (throw e))))

    :send (let [[f k v]   mop
                topic     (k->topic k)
                ; Create topic if it doesn't exist yet
                _         (when-not (contains? @extant-topics topic)
                            (rc/create-topic! admin topic partition-count
                                              replication-factor)
                            (swap! extant-topics conj topic))
                ; Send message to Redpanda
                partition (k->partition k)
                record (rc/producer-record topic (k->partition k) nil v)
                res    ^RecordMetadata @(.send producer record)
                k'     (topic-partition->k (.topic res)
                                           (.partition res))
                offset (when (.hasOffset res)
                         (.offset res))]
            (assert+ (= k k')
                     {:type ::key-mismatch
                      :k  k
                      :k' k'})
            ; As it turns out offsets go missing fairly often
            (when-not offset
              (info "Missing offset for send() acknowledgement of key" k "value" v))
            ;(assert+ offset
            ;         {:type ::missing-offset
            ;          :res  res})
            ; And record offset and value.
            [f k' [offset v]])))

(defrecord Client [; What node are we bound to?
                   node
                   ; Our three Kafka clients
                   ^Admin admin
                   ^KafkaProducer producer
                   ^KafkaConsumer consumer
                   ; An atom with a set of topics we've created. We have to
                   ; create topics before they can be used.
                   extant-topics]
  client/Client
  (open! [this test node]
    (let [tx-id (rc/new-transactional-id)]
      (assoc this
             :node      node
             :admin     (rc/admin test node)
;             :producer  (rc/producer (assoc test :transactional-id tx-id) node)
             :producer  (rc/producer test node)
             :consumer  (rc/consumer test node))))

  (setup! [this test])

  (invoke! [this test op]
    (case (:f op)
      ; Assign this consumer new topic-partitions
      :assign (do (.assign consumer (map k->topic-partition (:value op)))
                  (assoc op :type :ok))

      ; Crash this client, forcing us to open a new client (and consumer etc)
      :crash     (assoc op :type :info)

      ; Get some debugging information about the partition distribution on this
      ; node
      :debug-topic-partitions
      (try+
        (try
          (let [tps (->> (:value op)
                         (pmap (fn [k]
                                 [k (db/topic-partition-state
                                      node (k->topic-partition k))]))
                         (into (sorted-map)))]
            (assoc op :type :ok, :value {:node    node
                                         :node-id (db/node-id test node)
                                         :partitions tps}))
          (catch java.util.concurrent.ExecutionException e
            (throw (util/ex-root-cause e))))
        (catch [:type :clj-http.client/unexceptional-status] e
          (assoc op :type :fail, :error (:body e)))
        (catch java.net.ConnectException _
          (assoc op :type :fail, :error :connection-refused)))

      ; Subscribe to the topics containing these keys
      :subscribe (do (->> (:value op)
                          (map k->topic)
                          distinct
                          (rc/subscribe! consumer))
                     (assoc op :type :ok))

      ; Apply poll/send transactions
      (:poll, :send, :txn)
      (let [; How should we interpret the operation completion :type when a
            ; single micro-op definitely fails?
            mop-fail-type (if (= 1 (count (:value op)))
                            ; If we only tried a single mop, we can just fail
                            ; the whole op.
                            :fail
                            ; TODO: when we do transactions, the failure of any
                            ; single mop can be safely interpreted as a :fail
                            ; of the entire op, since all mops should fail to
                            ; commit as a unit. Without txns, these should be
                            ; info, because earlier writes may have committed.
                            :info)]
        (try
          (rc/unwrap-errors
            ;(.beginTransaction producer)
            (let [txn  (:value op)
                  txn' (mapv (partial mop! this (:poll-ms op poll-ms)) txn)]
              ; If we read, AND we're using :subscribe instead of assign,
              ; commit offsets. My understanding is that with assign you're not
              ; supposed to use the offset system?
              (when (and (#{:poll :txn} (:f op))
                         (:subscribe (:sub-via test)))
                (try (.commitSync consumer)
                     ; If we crash during commitSync *outside* a transaction,
                     ; it might be that we poll()ed some values in this txn
                     ; which Kafka will think we consumed. We won't have
                     ; any record of them if we fail the txn. Instead, we
                     ; return an :ok txn *with* the reads, but note the lack of
                     ; commit.
                     (catch RuntimeException e
                       (assoc op :type :ok, :value txn',
                              :error [:consumer-commit (.getMessage e)]))))
              ; TODO: enable txns, write some kind of macro for commit/abort.
              ;(.commitTransaction producer)
              ;(.abortTransaction producer)
              (assoc op :type :ok, :value txn')))
          (catch DisconnectException e
            (assoc op :type :info, :error [:disconnect (.getMessage e)]))

          (catch InvalidTopicException _
            (assoc op :type mop-fail-type, :error :invalid-topic))

          (catch InvalidReplicationFactorException _
            (assoc op :type mop-fail-type :error :invalid-replication-factor))

          (catch NetworkException e
            (assoc op :type :info, :error [:network (.getMessage e)]))

          (catch NotControllerException e
            (assoc op :type mop-fail-type, :error :not-controller))

          (catch NotLeaderOrFollowerException _
            (assoc op :type mop-fail-type, :error :not-leader-or-follower))

          (catch UnknownTopicOrPartitionException _
            (assoc op :type mop-fail-type, :error :unknown-topic-or-partition))

          (catch UnknownServerException e
            (assoc op :type :info, :error [:unknown-server-exception
                                           (.getMessage e)]))

          (catch TimeoutException _
            (assoc op :type :info, :error :timeout))))))

  (teardown! [this test])

  (close! [this test]
    (rc/close! admin)
    (rc/close! producer)
    (rc/close! consumer))

  client/Reusable
  (reusable? [this test]
    ; Later we might want to explicitly crash and tear down our client.
    false))

(defn client
  "Constructs a fresh client for this workload."
  []
  (map->Client {:extant-topics (atom #{})}))

;; Generator

(defn txn-generator
  "Takes a list-append generator and rewrites its transactions to be [:poll] or
  [:send k v] micro-ops. Also adds a :keys field onto each operation, with a
  set of keys that txn would have interacted with; we use this to generate
  :subscribe ops later."
  [la-gen]
  (gen/map (fn rewrite-op [op]
             (-> op
                 (assoc :keys (set (map second (:value op))))
                 (update :value
                         (partial mapv (fn rewrite-mop [[f k v]]
                                         (case f
                                           :append [:send k v]
                                           :r      [:poll]))))))
           la-gen))

(def subscribe-ratio
  "How many subscribe ops should we issue per txn op?"
  1/8)

(defrecord InterleaveSubscribes [gen]
  gen/Generator
  (op [this test context]
    ; When we're asked for an operation, ask the underlying generator for
    ; one...
    (when-let [[op gen'] (gen/op gen test context)]
      (if (= :pending op)
        [:pending this]
        (let [this' (InterleaveSubscribes. gen')]
          (if (< (rand) subscribe-ratio)
            ; At random, emit a subscribe/assign op instead.
            (let [f  (rand-nth (vec (:sub-via test)))
                  op {:f f, :value (vec (:keys op))}]
              [(gen/fill-in-op op context) this])
            ; Or pass through the op directly
            [(dissoc op :keys) (InterleaveSubscribes. gen')])))))

  ; Pass through updates
  (update [this test context event]
    (InterleaveSubscribes. (gen/update gen test context event))))

(defn interleave-subscribes
  "Takes a txn generator and keeps track of the keys flowing through it,
  interspersing occasional :subscribe or :assign operations for recently seen
  keys."
  [txn-gen]
  (InterleaveSubscribes. txn-gen))

(defn tag-rw
  "Takes a generator and tags operations as :f :poll or :send if they're
  entirely comprised of send/polls."
  [gen]
  (gen/map (fn tag-rw [op]
             (case (->> op :value (map first) set)
               #{:poll}  (assoc op :f :poll)
               #{:send}  (assoc op :f :send)
               op))
           gen))

(defn op->max-offsets
  "Takes an operation (presumably, an OK one) and returns a map of keys to the
  highest offsets interacted with in that op."
  [{:keys [type f value]}]
  (case type
    (:info, :ok)
    (case f
      (:poll, :send, :txn)
      (->> value
           (map (fn [[f :as mop]]
                  (case f
                    :poll (->> (second mop)
                               (map-vals (fn [pairs]
                                           (->> pairs
                                                (map first)
                                                (remove nil?)
                                                (reduce max -1)))))
                    :send (let [[_ k v] mop]
                            (when (and (vector? v) (first v))
                              {k (first v)})))))
           (reduce (partial merge-with max)))

      nil)
    nil))

(defrecord TrackKeyOffsets [gen offsets]
  gen/Generator
  (op [this test context]
    (when-let [[op gen'] (gen/op gen test context)]
      (if (= :pending op)
        [:pending this]
        [op (TrackKeyOffsets. gen' offsets)])))

  (update [this test context event]
    (let [op-offsets (op->max-offsets event)]
      (when-not (empty? op-offsets)
        (swap! offsets #(merge-with max % op-offsets))))
    (TrackKeyOffsets.
      (gen/update gen test context event) offsets)))

(defn track-key-offsets
  "Wraps a generator. Keeps track of every key that generator touches in the
  given atom, which is a map of keys to highest offsets seen."
  [keys-atom gen]
  (TrackKeyOffsets. gen keys-atom))

(defrecord FinalPolls [target-offsets]
  gen/Generator
  (op [this test context]
    ;(info "waiting for" target-offsets)
    (when-not (empty? target-offsets)
      [(gen/fill-in-op {:f :poll, :value [[:poll]], :poll-ms 1000} context)
       this]))

  (update [this test context {:keys [type f value] :as event}]
    ; (info :update event)
    (if (and (= type  :ok)
             (= f     :poll))
      (let [offsets' (reduce (fn [target-offsets' [k seen-offset]]
                               (if (<= (get target-offsets' k -1)
                                       seen-offset)
                                 ; We've read past our target offset for
                                 ; this key
                                 (dissoc target-offsets' k)
                                 target-offsets'))
                             target-offsets
                             (op->max-offsets event))]
        (when-not (identical? target-offsets offsets')
          (info "Process" (:process event) "now waiting for" offsets'))
        (FinalPolls. offsets'))
      ; Not relevant
      this)))

(defn final-polls
  "Takes an atom containing a map of keys to offsets. Constructs a generator
  which:

  1. Checks the topic-partition state from the admin API

  2. Crashes the client, to force a fresh one to be opened, just in case
     there's broken state inside the client.

  3. Assigns the new client to poll every key

  4. Polls until caught up to the offsets we think exist"
  [offsets]
  (delay
    (let [offsets @offsets]
      (info "Polling up to offsets" offsets)
      [{:f :debug-topic-partitions, :value (keys offsets)}
       {:f :crash}
       {:f :assign, :value (keys offsets)}
       (FinalPolls. offsets)])))

(defn crash-client-gen
  "A generator which, if the test has :crash-clients true, periodically emits
  an operation to crash a random client."
  [opts]
  (when (:crash-clients opts)
    (->> (repeat {:f :crash})
         (gen/stagger (/ (:crash-client-interval opts)
                         (:concurrency opts))))))

;; Checker ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(defn assocv
  "An assoc on vectors which allows you to assoc at arbitrary indexes, growing
  the vector as needed. When v is nil, constructs a fresh vector rather than a
  map."
  [v i value]
  (if v
    (if (<= i (count v))
      (assoc v i value)
      (let [nils (repeat (- i (count v)) nil)]
        (assoc (into v nils) i value)))
    ; Nil is treated as an empty vector.
    (recur [] i value)))

(defn nth+
  "Nth for vectors, but returns nil instead of out-of-bounds."
  [v i]
  (when (< i (count v))
    (nth v i)))

(defn version-orders-update-log
  "Updates a version orders log with the given offset and value."
  [log offset value]
  (if-let [values (nth+ log offset)]
    (assoc  log offset (conj values value)) ; Already have values
    (assocv log offset #{value}))) ; First time we've seen this offset

(defn version-orders-reduce-mop
  "Takes a logs object from version-orders and a micro-op, and integrates that
  micro-op's information about offsets into the logs."
  [logs mop]
  (case (first mop)
    :send (let [[_ k v] mop]
            (if (vector? v)
              (let [[offset value] v]
                (if offset
                  ; We completed the send and know an offset
                  (update logs k version-orders-update-log offset value)
                  ; Not sure what the offset was
                  logs))
              ; Not even offset structure: maybe an :info txn
              logs))

    :poll (reduce (fn poll-key [logs [k pairs]]
                    (reduce (fn pair [logs [offset value]]
                              (if offset
                                (update logs k version-orders-update-log
                                        offset value)
                                logs))
                            logs
                            pairs))
                  logs
                  (second mop))))

(defn index-seq
  "Takes a seq of distinct values, and returns a map of:

    {:by-index    A vector of the sequence
     :by-value    A map of values to their indices in the vector.}"
  [xs]
  {:by-index (vec xs)
   :by-value (into {} (map-indexed (fn [i x] [x i]) xs))})

(defn log->value->first-index
  "Takes a log: a vector of sets of read values for each offset in a partition,
  possibly including `nil`s. Returns a map which takes a value to the index
  where it first appeared."
  [log]
  (->> (remove nil? log)
       (reduce (fn [[earliest i] values]
                 [(reduce (fn [earliest value]
                            (if (contains? earliest value)
                              earliest
                              (assoc earliest value i)))
                          earliest
                          values)
                  (inc i)])
               [{} 0])
       first))

(defn log->last-index->values
  "Takes a log: a vector of sets of read values for each offset in a partition,
  possibly including `nil`s. Returns a vector which takes indices (dense
  offsets) to sets of values whose *last* appearance was at that position."
  [log]
  (->> (remove nil? log)
       ; Build up a map of values to their latest indexes
       (reduce (fn latest [[latest i] values]
                 [(reduce (fn [latest value]
                            (assoc latest value i))
                          latest
                          values)
                  (inc i)])
               [{} 0])
       first
       ; Then invert that map into a vector of indexes to sets
       (reduce (fn [log [value index]]
                 (let [values (get log index #{})]
                   (assocv log index (conj values value))))
               [])))

(defn version-orders
  "Takes a history and constructs a map of:

  {:orders   A map of keys to orders for that key. Each order is a map of:
               {:by-index        A vector which maps indices to single values,
                                 in log order.
                :by-value        A map of values to indices in the log.
                :log             A vector which maps offsets to sets of values
                                 in log order.}

   :errors   A series of error maps describing any incompatible orders, where
             a single offset for a key maps to multiple values.}

  Offsets are directly from Kafka. Indices are *dense* offsets, removing gaps
  in the log."
  ([history]
   (version-orders history {}))
  ([history logs]
   ; Logs is a map of keys to vectors, where index i in one of those vectors is
   ; the vector of all observed values for that index.
   (if (seq history)
     (let [op       (first history)
           history' (next history)]
       (case (:type op)
         ; There's nothing we can tell from invokes or fails
         (:invoke, :fail) (recur history' logs)

         ; Infos... right now nothing but we MIGHT be able to get partial info
         ; later, so we'll process them anyway.
         (case (:f op)
           (:poll, :send, :txn)
           (recur history' (reduce version-orders-reduce-mop logs (:value op)))
           ; Some non-transactional op, like an assign/subscribe
           (recur history' logs))))
     ; All done; transform our logs to orders.
     {:errors (->> logs
                   (mapcat (fn errors [[k log]]
                             (->> log
                                  (reduce (fn [[offset index errs] values]
                                            (condp <= (count values)
                                              ; Divergence
                                              2 [(inc offset) (inc index)
                                                 (conj errs {:key    k
                                                             :offset offset
                                                             :index  index
                                                             :values values})]
                                              ; No divergence
                                              1 [(inc offset) (inc index) errs]
                                              ; Hole in log
                                              0 [(inc offset) index errs]))
                                          [0 0 []])
                                  last)))
                   seq)
      :orders
      (map-vals
        (fn key-order [log]
          (assoc (->> log (remove nil?) (map first) index-seq)
                 :log log))
        logs)})))

(defn op-writes-helper
  "Takes an operation and a function which takes an offset-value pair. Returns
  a map of keys written by this operation to the sequence of (f [offset value])
  sends for that key. Note that offset may be nil."
  [op f]
  (when (#{:txn :send} (:f op))
    (reduce (fn [writes mop]
              (if (= :send (first mop))
                (let [[_ k v] mop
                      vs (get writes k [])
                      ; Values can be either a literal value or a [offset value]
                      ; pair.
                      value (if (vector? v)
                              (f v)
                              (f [nil v]))]
                  (assoc writes k (conj vs value)))
                ; Something other than a send
                writes))
            {}
            (:value op))))

(defn op-writes
  "Returns a map of keys to the sequence of all values written to that key in
  an op."
  [op]
  (op-writes-helper op second))

(defn op-write-offsets
  "Returns a map of keys to the sequence of all offsets written to that key in
  an op."
  [op]
  (op-writes-helper op first))

(defn op-write-pairs
  "Returns a map of keys to the sequence of all [offset value] pairs written to
  that key in an op."
  [op]
  (op-writes-helper op identity))

(defn op-reads-helper
  "Takes an operation and a function which takes an offset-value pair. Returns
  a map of keys read by this operation to the sequence of (f [offset value])
  read for that key."
  [op f]
  (when (#{:txn :poll} (:f op))
    (reduce (fn mop [res mop]
              (if (= :poll (first mop))
                (reduce (fn per-key [res [k pairs]]
                          (let [vs  (get res k [])
                                vs' (into vs (map f pairs))]
                            (assoc res k vs')))
                        res
                        (second mop))
                res))
            {}
            (:value op))))

(defn op-read-pairs
  "Returns a map of keys to the sequence of all [offset value] pairs read for
  that key."
  [op]
  (op-reads-helper op identity))

(defn op-read-offsets
  "Returns a map of keys to the sequence of all offsets read for that key."
  [op]
  (op-reads-helper op first))

(defn op-reads
  "Returns a map of keys to the sequence of all values read for that key."
  [op]
  (op-reads-helper op second))

(defn reads-of-key
  "Returns a seq of all operations which read the given key, and, optionally,
  read the given value."
  ([k history]
   (->> history
        (filter (comp #{:txn :send :poll} :f))
        (filter (fn [op]
                  (contains? (op-reads op) k)))))
  ([k v history]
   (->> history
        (reads-of-key k)
        (filter (fn [op]
                  (some #{v} (get (op-reads op) k)))))))

(defn writes-of-key
  "Returns a seq of all operations which wrote the given key, and, optionally,
  sent the given value."
  ([k history]
   (->> history
        (filter (comp #{:txn :send :poll} :f))
        (filter (fn [op]
                  (contains? (op-writes op) k)))))
  ([k v history]
   (->> history
        (writes-of-key k)
        (filter (fn [op]
                  (some #{v} (get (op-writes op) k)))))))

(defn reads-of-key-offset
  "Returns a seq of all operations which read the given key and offset."
  [k offset history]
  (->> history
       (reads-of-key k)
       (filter (fn [op]
                 (some #{offset} (get (op-read-offsets op) k))))))

(defn writes-of-key-offset
  "Returns a seq of all operations which wrote the given key and offset."
  [k offset history]
  (->> history
       (writes-of-key k)
       (filter (fn [op]
                 (some #{offset} (get (op-write-offsets op) k))))))

(defn reads-of-key-value
  "Returns a seq of all operations which read the given key and value."
  [k value history]
  (->> history
       (reads-of-key k)
       (filter (fn [op] (some #{value} (get (op-reads op) k))))))

(defn writes-of-key-value
  "Returns a seq of all operations which wrote the given key and value."
  [k value history]
  (->> history
       (writes-of-key k)
       (filter (fn [op] (some #{value} (get (op-writes op) k))))))

(defn op-around-key-offset
  "Takes an operation and returns that operation with its value trimmed so that
  any send/poll operations are constrained to just the given key, and values
  within n of the given offset. Returns nil if operation is not relevant."
  ([k offset op]
   (op-around-key-offset k offset 3 op))
  ([k offset n op]
   (when (and (= :ok (:type op))
              (#{:send :poll :txn} (:f op)))
     (let [value'
           (keep (fn [[f v :as mop]]
                   (case f
                     :poll (when-let [pairs (get v k)]
                             ; Trim pairs to region around offset
                             (let [trimmed
                                   (filter (fn [[o v]]
                                             (<= (- offset n) o (+ offset n)))
                                           pairs)]
                               (when (seq trimmed)
                                 [:poll {k trimmed}])))
                     :send (let [[_ k2 [o v]] mop]
                             (when (and (= k k2)
                                        (<= (- offset n) o (+ offset n)))
                               mop))))
                 (:value op))]
       (when-not (empty? value')
         (assoc op :value value'))))))

(defn around-key-offset
  "Filters a history to just those operations around a given key and offset;
  trimming their mops to just those regions as well."
  ([k offset history]
   (around-key-offset k offset 3 history))
  ([k offset n history]
   (keep (partial op-around-key-offset k offset n) history)))

(defn around-some
  "Clips a sequence to just those elements near a predicate. Takes a predicate,
  a range n, and a sequence xs. Returns the series of all x in xs such x is
  within n elements of some x' matching predicate."
  [pred n coll]
  (let [indices (first
                  (reduce (fn [[indices i] x]
                            (if (pred x)
                              [(into indices (range (- i n) (+ i n 1))) (inc i)]
                              [indices (inc i)]))
                          [#{} 0]
                          coll))]
    (first (reduce (fn [[out i] x]
                     (if (indices i)
                       [(conj out x) (inc i)]
                       [out (inc i)]))
                   [[] 0]
                   coll))))

(defn op-around-key-value
  "Takes an operation and returns that operation with its value trimmed so that
  any send/poll operations are constrained to just the given key, and values
  within n of the given value. Returns nil if operation is not relevant."
  ([k value op]
   (op-around-key-value k value 3 op))
  ([k value n op]
   (when (and (= :ok (:type op))
              (#{:send :poll :txn} (:f op)))
     (let [value'
           (keep (fn [[f v :as mop]]
                   (case f
                     :poll (when-let [pairs (get v k)]
                             ; Trim pairs to region around offset
                             (let [trimmed (around-some (comp #{value} second)
                                                        n pairs)]
                               (when (seq trimmed)
                                 {k trimmed})))
                     :send (let [[_ k2 [o v]] mop]
                             (when (and (= k k2) (= value v))
                               mop))))
                 (:value op))]
       (when-not (empty? value')
         (assoc op :value value'))))))

(defn around-key-value
  "Filters a history to just those operations around a given key and value;
  trimming their mops to just those regions as well."
  ([k value history]
   (around-key-value k value 3 history))
  ([k value n history]
   (keep (partial op-around-key-value k value n) history)))

(defn writes-by-type
  "Takes a history and constructs a map of types (:ok, :info, :fail) to maps of
  keys to the set of all values which were written for that key. We use this to
  identify, for instance, what all the known-failed writes were for a given
  key."
  [history]
  (->> history
       (remove (comp #{:invoke} :type))
       (filter (comp #{:txn :send} :f))
       (group-by :type)
       (map-vals (fn [ops]
                   (->> ops
                        ; Construct a seq of {key [v1 v2 ...]} maps
                        (map op-writes)
                        ; And turn [v1 v2 ...] into #{v1 v2 ...}
                        (map (partial map-vals set))
                        ; Then merge them all together
                        (reduce (partial merge-with set/union) {}))))))

(defn reads-by-type
  "Takes a history and constructs a map of types (:ok, :info, :fail) to maps of
  keys to the set of all values which were read for that key. We use this to
  identify, for instance, the known-successful reads for some key as a part of
  finding lost updates."
  [history]
  (->> history
       (remove (comp #{:invoke} :type))
       (filter (comp #{:txn :poll} :f))
       (group-by :type)
       (map-vals (fn [ops]
                   (->> ops
                        (map op-reads)
                        (map (partial map-vals set))
                        (reduce (partial merge-with set/union) {}))))))

(defn g1a-cases
  "Takes a partial analysis and looks for aborted reads, where a known-failed
  write is nonetheless visible to a committed read. Returns a seq of error
  maps, or nil if none are found."
  [{:keys [history writes-by-type]}]
  (let [failed (:fail writes-by-type)
        ops (->> history
                 (filter (comp #{:ok} :type))
                 (filter (comp #{:txn :poll} :f)))]
    (->> (for [op     ops
               [k vs] (op-reads op)
               v      vs
               :when (contains? (get failed k) v)]
           {:op    op
            :key   k
            :value v})
         seq)))

(defn lost-update-cases
  "Takes a partial analysis and looks for cases of lost update: where a write
  that we *should* have observed is somehow not observed. Of course we cannot
  expect to observe everything: for example, if we send a message to Redpanda
  at the end of a test, and don't poll for it, there's no chance of us seeing
  it at all! Or a poller could fall behind.

  What we do instead is identify the highest read value for each key v_max, and
  then take the set of all values *prior* to it in the version order: surely,
  if we read v_max = 3, and the version order is [1 2 3 4], we should also have
  read 1 and 2.

  It's not *quite* this simple. If a message appears at multiple offsets, the
  version order will simply pick one for us, which leads to nondeterminism. If
  an offset has multiple messages, a successfully inserted message could appear
  *nowhere* in the version order.

  To deal with this, we examine the raw logs for each key, and build two index
  structures. The first maps values to their earliest (index) appearance in the
  log: we use this to determine the highest index that must have been read. The
  second is a vector which maps indexes to sets of values whose *last*
  appearance in the log was at that index. We use this vector to identify which
  values ought to have been read.

  Once we've derived the set of values we ought to have read for some key k, we
  run through each poll of k and cross off the values read. If there are any
  values left, they must be lost updates."
  [{:keys [history version-orders reads-by-type]}]
  ; Start with all the values we know were read
  (->> (:ok reads-by-type)
       (keep (fn [[k vs]]
              ; Great, now for this key, find the highest index observed
              (let [vo         (get version-orders k)
                    ; For each value, what's the earliest index we observed
                    ; that value at?
                    value->first-index (log->value->first-index (:log vo))
                    ; And for each index, which values appeared at that index
                    ; *last*?
                    last-index->values (log->last-index->values (:log vo))

                    ;_ (info :k k :vs vs :vo vo)
                    ; From each value, we take the latest of the earliest
                    ; indices that value appeared at.
                    bound (->> vs
                               ; We might observe a value but *not* know
                               ; its offset from either write or read.
                               ; When this happens, we can't say anything
                               ; about how much of the partition should
                               ; have been observed, so we skip it.
                               (keep value->first-index)
                               (reduce max -1))
                    ; Now take a prefix of last-index->values up to that
                    ; index; these are the values we should have observed.
                    must-read (->> (inc bound)
                                   (subvec last-index->values 0)
                                   ; This is a vector of sets. We glue them
                                   ; together this way to preserve index order.
                                   (mapcat identity)
                                   distinct)
                    ; Now we go *back* to the read values vs, and strip them
                    ; out from the must-read set; anything left is something we
                    ; failed to read.
                    lost (remove vs must-read)]
                (when (seq lost)
                  {:key  k
                   :lost lost}))))
       seq))

(defn strip-types
  "Takes a collection of maps, and removes their :type fields. Returns nil if
  none remain."
  [ms]
  (seq (map (fn [m] (dissoc m :type)) ms)))

(defn int-poll-skip+nonmonotonic-cases
  "Takes a partial analysis and looks for cases where a single transaction
  contains:

    {:skip          A pair of poll values which read the same key and skip
                    over some part of the log which we know should exist.
     :nonmonotonic A pair of poll values which *contradict* the log order,
                   or repeat the same value.}"
  [{:keys [history version-orders]}]
  (->> history
       (pmap (fn per-op [op]
                 ; Consider each pair of reads of some key in this op...
                 (->> (for [[k vs]  (op-reads op)
                            [v1 v2] (partition 2 1 vs)]
                        (let [{:keys [by-index by-value]} (get version-orders k)
                              ; What are their indices in the log?
                              i1 (get by-value v1)
                              i2 (get by-value v2)
                              delta (if (and i1 i2)
                                      (- i2 i1)
                                      1)]
                          (cond (< 1 delta)
                                {:type     :skip
                                 :key      k
                                 :values   [v1 v2]
                                 :delta    delta
                                 :skipped  (map by-index (range (inc i1) i2))
                                 :op       op}

                                (< delta 1)
                                {:type    :nonmonotonic
                                 :key     k
                                 :values  [v1 v2]
                                 :delta   delta
                                 :op      op})))
                      (remove nil?))))
       (mapcat identity)
       (group-by :type)
       (map-vals strip-types)))

(defn int-send-skip+nonmonotonic-cases
  "Takes a partial analysis and looks for cases where a single transaction
  contains a pair of sends to the same key which:

    {:skip          Skips over some indexes of the log
     :nonmonotonic  Go backwards (or stay in the same place) in the log}"
  [{:keys [history version-orders]}]
  (->> history
       (mapcat (fn per-op [op]
                 ; Consider each pair of sends to a given key in this op...
                 (->> (for [[k vs]  (op-writes op)
                            [v1 v2] (partition 2 1 vs)]
                        (let [{:keys [by-index by-value]} (get version-orders k)
                              i1 (get by-value v1)
                              i2 (get by-value v2)
                              delta (if (and i1 i2)
                                      (- i2 i1)
                                      1)]
                          (cond (< 1 delta)
                                {:type    :skip
                                 :key     :x
                                 :values  [v1 v2]
                                 :delta   delta
                                 :skipped (map by-index (range (inc i1) i2))
                                 :op      op}

                                (< delta 1)
                                {:type   :nonmonotonic
                                 :key    k
                                 :values [v1 v2]
                                 :delta  delta
                                 :op     op})))
                      (remove nil?))))
       (group-by :type)
       (map-vals strip-types)))

(defn poll-skip+nonmonotonic-cases
  "Takes a partial analysis and checks each process's operations sequentially,
  looking for cases where a single process either jumped backwards or skipped
  over some region of a topic-partition. Returns a map:

    {:nonmonotonic  Cases where a process started polling at or before a
                    previous operation last left off
     :skip          Cases where two successive operations by a single process
                    skipped over one or more values for some key.}"
  [{:keys [history version-orders]}]
  ; First, group ops by process
  (->> (group-by :process history)
       ; Then reduce each process, keeping track of the most recent read
       ; index for each key.
       (map-vals
         (fn per-process [ops]
           ; Iterate over this process's operations, building up a vector of
           ; errors.
           (loop [ops        ops
                  errors     []
                  last-reads {}]
             (if-not (seq ops)
               ; Done
               errors
               ; Process this op
               (let [op    (first ops)]
                 (case (:f op)
                   ; When we assign or subscribe a new topic, preserve *only*
                   ; those last-reads which we're still subscribing to.
                   (:assign, :subscribe)
                   (recur (next ops)
                          errors
                          (if (#{:invoke :fail} (:type op))
                            last-reads
                            (select-keys last-reads (:value op))))

                   (:txn, :poll)
                   (let [reads (op-reads op)
                         ; Look at each key and values read by this op.
                         errs (for [[k reads] reads]
                                ; What was the last op that read this key?
                                (if-let [last-op (get last-reads k)]
                                  ; Compare the index of the last thing read by
                                  ; the last op read to the index of our first
                                  ; read
                                  (let [vo  (get-in version-orders
                                                    [k :by-value])
                                        v   (-> last-op op-reads (get k) last)
                                        v'  (first reads)
                                        i   (get vo v)
                                        i'  (get vo v')
                                        delta (if (and i i')
                                                (- i' i)
                                                1)]
                                    [(when (< 1 delta)
                                       ; We can show that this op skipped an
                                       ; index!
                                       (let [voi (-> version-orders (get k)
                                                     :by-index)
                                             skipped (map voi
                                                          (range (inc i) i'))]
                                         {:type    :skip
                                          :key     k
                                          :delta   delta
                                          :skipped skipped
                                          :ops     [last-op op]}))
                                     (when (< delta 1)
                                       ; Aha, this wasn't monotonic!
                                       {:type   :nonmonotonic
                                        :key    k
                                        :values [v v']
                                        :delta  delta
                                        :ops    [last-op op]})])
                                  ; First read of this key
                                  nil))
                         errs (->> errs
                                   (mapcat identity)
                                   (remove nil?))
                         ; Update our last-reads index for this op's read keys
                         last-reads' (->> (keys reads)
                                          (reduce (fn update-last-read [lr k]
                                                    (assoc lr k op))
                                                  last-reads))]
                     (recur (next ops) (into errors errs) last-reads'))

                   ; Some other :f
                   (recur (next ops) errors last-reads)))))))
           ; Join together errors from all processes
           (mapcat val)
           (group-by :type)
       (map-vals strip-types)))

(defn nonmonotonic-send-cases
  "Takes a partial analysis and checks each process's operations sequentially,
  looking for cases where a single process's sends to a given key go backwards
  relative to the version order."
  [{:keys [history version-orders]}]
  ; First, consider each process' completions...
  (->> (filter (comp #{:ok :info} :type) history)
       (group-by :process)
       ; Then reduce each process, keeping track of the most recent sent
       ; index for each key.
       (map-vals
         (fn per-process [ops]
           ; Iterate over this process's operations, building up a vector of
           ; errors.
           (loop [ops        ops
                  errors     []
                  last-sends {}] ; Key -> Most recent op to send to that key
             (if-not (seq ops)
               ; Done
               errors
               ; Process this op
               (let [op (first ops)]
                 (case (:f op)
                   ; When we assign or subscribe a new topic, preserve *only*
                   ; those last-reads which we're still subscribing to.
                   (:assign, :subscribe)
                   (recur (next ops)
                          errors
                          (if (#{:invoke :fail} (:type op))
                            last-sends
                            (select-keys last-sends (:value op))))

                   (:send, :txn)
                   (let [; Look at each key and values sent by this op.
                         sends (op-writes op)
                         errs (for [[k sends] sends]
                                ; What was the last op that sent to this key?
                                (if-let [last-op (get last-sends k)]
                                  ; Compare the index of the last thing sent by
                                  ; the last op to the index of our first send
                                  (let [vo  (get-in version-orders
                                                    [k :by-value])
                                        v   (-> last-op op-writes (get k) last)
                                        v'  (first sends)
                                        i   (get vo v)
                                        i'  (get vo v')
                                        delta (if (and i i')
                                                (- i' i)
                                                1)]
                                    (when (< delta 1)
                                      ; Aha, this wasn't monotonic!
                                      {:key    k
                                       :values [v v']
                                       :delta  (- i' i)
                                       :ops    [last-op op]}))
                                  ; First send to this key
                                  nil))
                         errs (->> errs
                                   (remove nil?))
                         ; Update our last-reads index for this op's sent keys
                         last-sends' (->> (keys sends)
                                          (reduce (fn update-last-send [lr k]
                                                    (assoc lr k op))
                                                  last-sends))]
                     (recur (next ops) (into errors errs) last-sends'))

                   ; Some other :f
                   (recur (next ops) errors last-sends)))))))
       ; Join together errors from all processes
       (mapcat val)
       seq))

(defn duplicate-cases
  "Takes a partial analysis and identifies cases where a single value appears
  at more than one offset in a key."
  [{:keys [version-orders]}]
  (->> version-orders
       (mapcat (fn per-key [[k version-order]]
                 (->> (:by-index version-order)
                      frequencies
                      (filter (fn dup? [[value number]]
                                (< 1 number)))
                      (map (fn [[value number]]
                             {:key    k
                              :value  value
                              :count  number})))))
       seq))

(defn unseen
  "Takes a history and yields a series of maps like

    {:time    The time in nanoseconds
     :unseen  A map of keys to the number of messages in that key which have
              been successfully acknowledged, but not polled by any client.}

  The final map in the series includes a :messages key: a map of keys to sets
  of messages that were unseen."
  [history]
  (let [[out unseen]
        (reduce
          ; Out is a vector of output observations. sent is a map of keys to
          ; successfully sent values; polled is a map of keys to successfully
          ; read values.
          (fn red [[out sent polled :as acc] {:keys [type time f] :as op}]
            (if-not (and (= type :ok)
                         (#{:poll :send :txn} f))
              acc
              (let [sent'   (->> op op-writes (map-vals set)
                                 (merge-with set/union sent))
                    polled' (->> op op-reads (map-vals set)
                                 (merge-with set/union polled))
                    unseen  (merge-with set/difference sent' polled')]
                ; We don't have to keep looking for things we've seen, so we can
                ; recur with unseen rather than sent'.
                [(conj! out {:time time, :unseen (map-vals count unseen)})
                 unseen
                 polled'])))
          [(transient []) {} {}]
          history)
        out (persistent! out)]
    (update out (dec (count out))
            assoc :messages unseen)))

(defn plot-unseen!
  "Takes a test, a collection of unseen measurements, and options (e.g. those
  to checker/check). Plots a graph file (unseen.png) in the store directory."
  [test unseen {:keys [subdirectory]}]
  (let [ks       (-> unseen peek :unseen keys sort reverse vec)
        ; Turn these into stacked graphs
        datasets (reduce
                   (fn [datasets {:keys [time unseen]}]
                     ; We go through keys in reverse order so that the last
                     ; key in the legend is on the bottom of the chart
                     (->> ks reverse
                          (reduce (fn [[datasets sum] k]
                                    (if-let [count (get unseen k 0)]
                                      (let [t        (nanos->secs time)
                                            sum'     (+ sum count)
                                            dataset  (get datasets k [])
                                            dataset' (conj dataset [t sum'])]
                                        [(assoc datasets k dataset') sum'])
                                      ; Key doesn't exist yet
                                      [datasets sum]))
                                  [datasets 0])
                          first))
                   {}
                   unseen)
        output   (.getCanonicalPath
                   (store/path! test subdirectory "unseen.png"))
        preamble (concat (perf/preamble output)
                         [[:set :title (str (:name test) " unseen")]
                          '[set ylabel "Unseen messages"]])
        series (for [k ks]
                 {:title (str "key " k)
                  :with '[filledcurves x1]
                  :data (datasets k)})]
    (-> {:preamble preamble, :series series}
        perf/with-range
        perf/plot!
        (try+ (catch [:type :jepsen.checker.perf/no-points] _ :no-points)))))

(defn realtime-lag
  "Takes a history and yields a series of maps of the form

    {:process The process performing a poll
     :key     The key being polled
     :time    The time the read began, in nanos
     :lag     The realtime lag of this key, in nanos.

  The lag of a key k in a poll is the conservative estimate of how long it has
  been since the highest value in that poll was the final message in log k.

  For instance, given:

    {:time 1, :type :ok, :value [:send :x [0 :a]]}
    {:time 2, :type :ok, :value [:poll {:x [0 :a]}]}

  The lag of this poll is zero, since we observed the most recent completed
  write to x. However, if we:

    {:time 3, :type :ok,      :value [:send :x [1 :b]]}
    {:time 4, :type :invoke,  :value [:poll]}
    {:time 5, :type :ok,      :value [:poll {:x []}]}

  The lag of this read is 4 - 3 = 1. By time 3, offset 1 must have existed for
  key x. However, the most recent offset we observed was 0, which could only
  have been the most recent offset up until the write of offset 1 at time 3.
  Since our read could have occurred as early as time 4, the lag is at least 1.

  Might want to make this into actual [lower upper] ranges, rather than just
  the lower bound on lag, but conservative feels OK for starters."
  [history]
  ; First, build up a map of keys to vectors, where element i of the vector for
  ; key k is the time at which we knew offset i+1 was no longer the most recent
  ; thing in the log. Thus the first entry in each vector is the time at which
  ; the log was no longer *empty*. The second entry is the time at which offset
  ; 0 was no longer current, and so on.
  (let [expired
        (reduce
          (fn expired [expired {:keys [type f time value] :as op}]
            ; What are the most recent offsets we know exist?
            (let [known-offsets (op->max-offsets op)]
              (reduce
                (fn [expired [k known-offset]]
                  ; For this particulary key, we know this offset exists.
                  ; Therefore we *also* know every lower offset should
                  ; exist. We zip backwards, filling in offsets with the
                  ; current time, until we hit an offset where a lower
                  ; time exists.
                  (loop [expired-k (get expired k [])
                         i         known-offset]
                    (if (or (neg? i) (get expired-k i))
                      ; We've gone back before the start of the vector, or
                      ; we hit an index that's already known to be expired
                      ; earlier.
                      (assoc expired k expired-k)
                      ; Update this index
                      (recur (assocv expired-k i time) (dec i)))))
                expired
                known-offsets)))
          {}
          history)
        pairs (history/pair-index+ history)]
    ; Now work through the history, turning each poll operation into a lag map.
    (loop [history    history
           ; A map of processes to keys to the highest offset that process has
           ; seen. Keys in this map correspond to the keys this process
           ; currently has :assign'ed. If using `subscribe`, keys are filled in
           ; only when they appear in poll operations. The default offset is
           ; -1.
           process-offsets {}
           ; A vector of output lag maps.
           lags []]
      (if-not (seq history)
        lags
        (let [[op & history'] history
              {:keys [type process f value]} op]
          (if-not (= :ok type)
            ; Only OK ops matter
            (recur history' process-offsets lags)

            (case f
              ; When we assign something, we replace the process's offsets map.
              :assign
              (recur history'
                     (let [offsets (get process-offsets process {})]
                       ; Preserve keys that we're still subscribing to;
                       ; anything else gets reset to offset -1. This is gonna
                       ; break if we use something other than auto_offset_reset
                       ; = earliest, but... deal with that later. I've got
                       ; limited time here and just need to get SOMETHING
                       ; working.
                       (assoc process-offsets process
                              (merge (zipmap value (repeat -1))
                                     (select-keys offsets value))))
                     lags)

              ; When we subscribe, we're not necessarily supposed to get
              ; updates for the key we subscribed to--we subscribe to *topics*,
              ; not individual partitions. We might get *no* updates, if other
              ; subscribers have already claimed all the partitions for that
              ; topic. We reset the offsets map to empty, to be conservative.
              :subscribe
              (recur history' (assoc process-offsets process {}) lags)

              (:poll, :txn)
              (let [invoke-time (:time (get pairs op))
                    ; For poll ops, we merge all poll mop results into this
                    ; process's offsets, then figure out how far behind each
                    ; key based on the time that key offset expired.
                    offsets' (merge-with max
                                         (get process-offsets process {})
                                         (op->max-offsets op))
                    lags' (->> offsets'
                               (map (fn [[k offset]]
                                      ; If we've read *nothing*, then we're at
                                      ; offset -1. Element 0 in the expired
                                      ; vector for k tells us the time when the
                                      ; first element was definitely present.
                                      ; If we read offset 0, we want to consult
                                      ; element 1 of the vector, which tells us
                                      ; when offset 0 was no longer the tail,
                                      ; and so on.
                                      (let [expired-k (get expired k [])
                                            i (inc offset)
                                            expired-at (get-in expired
                                                               [k (inc offset)]
                                                               ##Inf)
                                            lag (-> invoke-time
                                                    (- expired-at)
                                                    (max 0))]
                                        {:time    invoke-time
                                         :process process
                                         :key     k
                                         :lag     lag})))
                               (into lags))]
                (recur history'
                       (assoc process-offsets process offsets')
                       lags'))

              ; Anything else, pass through
              (recur history' process-offsets lags))))))))

(defn op->thread
  "Returns the thread which executed a given operation."
  [test op]
  (-> op :process (mod (:concurrency test))))

(defn plot-realtime-lag!
  "Takes a test, a collection of realtime lag measurements, and options (e.g.
  those to checker/check). Plots a graph file (realtime-lag.png) in the store
  directory"
  [test lags {:keys [nemeses subdirectory filename
                     group-fn group-name]}]
  (let [nemeses  (or nemeses (:nemeses (:plot test)))
        datasets (->> lags
                      (group-by group-fn)
                      (map-vals (fn [lags]
                                  ; At any point in time, we want the maximum
                                  ; lag for this thread across any key.
                                  (->> lags
                                       (partition-by :time)
                                       (map (partial util/max-by :lag))
                                       (map (juxt (comp nanos->secs :time)
                                                  (comp nanos->secs :lag)))))))
        output  (.getCanonicalPath
                  (store/path! test subdirectory
                               (or filename "realtime-lag.png")))
        preamble (concat (perf/preamble output)
                         [[:set :title (str (:name test)
                                            " realtime lag by "
                                            group-name)]
                          '[set ylabel "Lag (s)"]])
        series (for [g (util/polysort (keys datasets))]
                 {:title (str group-name " " g)
                  :with 'linespoints
                  :data (datasets g)})]
    (-> {:preamble preamble
         :series series}
        perf/with-range
        perf/plot!
        (try+ (catch [:type :jepsen.checker.perf/no-points] _ :no-points)))))

(defn plot-realtime-lags!
  "Constructs realtime lag plots for all processes together, and then another
  broken out by process, and also by key."
  [test lags opts]
  (->> [{:group-name "thread",     :group-fn (partial op->thread test)}
        {:group-name "key",        :group-fn :key}
        {:group-name "thread-key", :group-fn (juxt (partial op->thread test)
                                                   :key)}]
       (mapv (fn [{:keys [group-name group-fn] :as opts2}]
               (let [opts (merge opts opts2)]
                 (plot-realtime-lag!
                   test lags (assoc opts :filename (str "realtime-lag-"
                                                        group-name ".png")))
                 (->> lags
                      (group-by group-fn)
                      (pmap (fn [[thread lags]]
                              (plot-realtime-lag!
                                test lags
                                (assoc opts
                                       :subdirectory (str "realtime-lag-"
                                                          group-name)
                                       :filename (str thread ".png")))))
                      dorun))))))

(defn worst-realtime-lag
  "Takes a seq of realtime lag measurements, and finds the point with the
  highest lag."
  [lags]
  (or (util/max-by :lag lags) 0))

(defn analysis
  "Builds up intermediate data structures used to understand a history."
  [history]
  (let [history               (history/index history)
        history               (remove (comp #{:nemesis} :process) history)
        version-orders        (future (version-orders history))
        writes-by-type        (future (writes-by-type history))
        reads-by-type         (future (reads-by-type history))
        ; Sort of a hack; we only bother computing this for "real" histories
        ; because our test suite often leaves off processes and times
        realtime-lag          (future
                                (let [op (first history)]
                                  (when (every? op [:process :time])
                                    (realtime-lag history))))
        worst-realtime-lag    (future (worst-realtime-lag @realtime-lag))
        unseen                (future (unseen history))
        version-order-errors  (:errors @version-orders)
        version-orders        (:orders @version-orders)
        analysis              {:history        history
                               :writes-by-type @writes-by-type
                               :reads-by-type  @reads-by-type
                               :version-orders version-orders}
        g1a-cases               (future (g1a-cases analysis))
        lost-update-cases       (future (lost-update-cases analysis))
        poll-skip+nm-cases      (future (poll-skip+nonmonotonic-cases analysis))
        nonmonotonic-send-cases (future (nonmonotonic-send-cases analysis))
        int-poll-skip+nm-cases  (future (int-poll-skip+nonmonotonic-cases analysis))
        int-send-skip+nm-cases  (future (int-send-skip+nonmonotonic-cases analysis))
        duplicate-cases         (future (duplicate-cases analysis))
        poll-skip-cases         (:skip @poll-skip+nm-cases)
        nonmonotonic-poll-cases (:nonmonotonic @poll-skip+nm-cases)
        int-poll-skip-cases     (:skip @int-poll-skip+nm-cases)
        int-nm-poll-cases       (:nonmonotonic @int-poll-skip+nm-cases)
        int-send-skip-cases     (:skip @int-send-skip+nm-cases)
        int-nm-send-cases       (:nonmonotonic @int-send-skip+nm-cases)
        ]
    {:errors (cond-> {}
               @duplicate-cases
               (assoc :duplicate @duplicate-cases)

               int-poll-skip-cases
               (assoc :int-poll-skip int-poll-skip-cases)

               int-nm-poll-cases
               (assoc :int-nonmonotonic-poll int-nm-poll-cases)

               int-nm-send-cases
               (assoc :int-nonmonotonic-send int-nm-send-cases)

               int-send-skip-cases
               (assoc :int-send-skip int-send-skip-cases)
               version-order-errors
               (assoc :inconsistent-offsets version-order-errors)

               @g1a-cases
               (assoc :g1a @g1a-cases)

               @lost-update-cases
               (assoc :lost-update @lost-update-cases)

               nonmonotonic-poll-cases
               (assoc :nonmonotonic-poll nonmonotonic-poll-cases)

               @nonmonotonic-send-cases
               (assoc :nonmonotonic-send @nonmonotonic-send-cases)

               poll-skip-cases
               (assoc :poll-skip poll-skip-cases)
               )
     :realtime-lag       @realtime-lag
     :worst-realtime-lag @worst-realtime-lag
     :unseen             @unseen
     :version-orders     version-orders}))

(defn condense-error
  "Takes a test and a  pair of an error type (e.g. :lost-update) and a seq of
  errors. Returns a pair of [type, {:count n, :errors [...]}], which tries to
  show the most interesting or severe errors without making the pretty-printer
  dump out two gigabytes of EDN."
  [test [type errs]]
  [type
   {:count (count errs)
    :errs
    (if (:all-errors test)
      errs
      (case type
        :duplicate             (take 32      (sort-by :count errs))
        :inconsistent-offsets  (take 32 (sort-by (comp count :values) errs))
        :int-nonmonotonic-poll (take 8       (sort-by :delta errs))
        :int-nonmonotonic-send (take 8       (sort-by :delta errs))
        :int-poll-skip         (take-last 8  (sort-by :delta errs))
        :int-send-skip         (take-last 8  (sort-by :delta errs))
        :nonmonotonic-poll     (take 8       (sort-by :delta errs))
        :nonmonotonic-send     (take 8       (sort-by :delta errs))
        :poll-skip             (take-last 8  (sort-by :delta errs))
        errs))}])

(defn checker
  []
  (reify checker/Checker
    (check [this test history opts]
      (let [{:keys [errors
                    realtime-lag
                    worst-realtime-lag
                    unseen] :as analysis} (analysis history)
            bad-errors    errors
            latest-unseen (-> unseen
                              peek
                              (dissoc :messages)
                              (update :time nanos->secs)
                              (update :unseen (partial into (sorted-map))))]
        ; Render plots
        (plot-unseen!        test unseen opts)
        (plot-realtime-lags! test realtime-lag opts)
        ; Construct results
        (->> errors
             (map (partial condense-error test))
             (into (sorted-map))
             (merge {:valid? (and (empty? bad-errors)
                                  (every? zero? (vals (:unseen latest-unseen))))
                     :unseen             latest-unseen
                     :worst-realtime-lag (-> worst-realtime-lag
                                             (update :time nanos->secs)
                                             (update :lag nanos->secs))
                     :error-types        (keys errors)}))))))

(defn workload
  "Constructs a workload (a map with a generator, client, checker, etc) given
  an options map. Options are:

    (none)

  ... plus those taken by jepsen.tests.cycle.append/test, e.g. :key-count,
  :min-txn-length, ..."
  [opts]
  (let [workload (append/test
                   (assoc opts
                          ; TODO: don't hardcode these
                          :max-txn-length 1
                          :max-writes-per-key 1024
                          :consistency-models [:strict-serializable]))
        max-offsets (atom (sorted-map))]
    (assoc workload
           :client          (client)
           :checker         (checker)
           :final-generator (gen/each-thread
                              ; (final-polls max-offsets)
                              (->> (gen/phases (final-polls max-offsets)
                                               (repeat (gen/sleep 1)))
                                   (gen/time-limit 10)
                                   repeat))
           :generator (gen/any
                        (crash-client-gen opts)
                        (->> (:generator workload)
                             txn-generator
                             tag-rw
                             (track-key-offsets max-offsets)
                             interleave-subscribes)))))
